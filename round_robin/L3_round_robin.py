# round_robin/L3_round_robin.py

import asyncio

from config import Config
from llm_call import llm_call
from bots.L3_bot import L3Bot


class L3RoundRobin:
    """
    A class representing a round-robin match between two L3 bots' responses.
    
    This class holds a pair of L3 bot instances (which share the same instruction)
    and is tasked with comparing their responses (stored in the attribute `subsection_draft`)
    to decide which response is better. The comparison is done asynchronously using llm_call.
    
    Attributes:
        bot1: The first L3 bot instance.
        bot2: The second L3 bot instance.
        result: The outcome of the match:
            - 1 if bot1's response is better,
            - 2 if bot2's response is better,
            - 0 if they are deemed equal (tie).
    """
    def __init__(self, bot1: L3Bot, bot2: L3Bot):
        self.bot1 = bot1
        self.bot2 = bot2
        self.result = None
        self.reasoning_output = ""

        # System prompts for the two LLM calls.
        self.system_prompt_reasoning = (
            "You are a Level 3 (L3) Evaluation Bot. Your task is to critically assess two "
            "responses generated by L3 bots. First, provide a detailed reasoning about the strengths "
            "and weaknesses of each response in terms of clarity, correctness, and adherence to the instructions. "
            "Be rigorous and objective in your analysis."
        )
        
        self.system_prompt_decision = (
            "You are a Level 3 (L3) Evaluation Bot. Based on the provided reasoning, decide which response is superior. "
            "Respond with exactly '1' if the first response is superior, '2' if the second response is superior, "
            "or 'TIE' if both responses are equally good."
        )

        # Control attributes for iterative processing
        self.done = False
        self.current_llm_call_index = 0


    async def _reason_competition(self):
        """
        Perform the initial evaluation by comparing the two responses.
        This asynchronous LLM call prompts the bot to provide detailed reasoning.
        """
        prompt = (
            "CURRENT DOCUMENT:\n\n"

            f"{self.bot1.document}\n\n"

            "SECTION INSTRUCTIONS:\n\n"

            f"{self.bot1.L2_instruction}\n\n"

            "CURRENT SECTION:\n\n"

            f"{self.bot1.section}\n\n"

            "SUBSECTION INSTRUCTIONS:\n\n"

            f"{self.bot1.L3_instruction}\n\n"

            "CURRENT SUBSECTION:\n\n"

            f"{self.bot1.subsection}\n\n"

            "SUBSECTION BLOCK 1:\n\n"

            f"{self.bot1.subsection_draft}\n\n"

            "SUBSECTION BLOCK 2:\n\n"

            f"{self.bot2.subsection_draft}\n\n"

            "You have been provided with the responses of two L3 bots above, i.e., "
            "subsection block 1 and subsection block 2. Reason about the strengths and weaknesses of "
            "each in terms of the quality of the subsection and adherence to the instructions. "
            "You can assume that the actual content is correct."
        )
        
        self.reasoning_output = await llm_call(
            prompt=prompt,
            system_prompt=self.system_prompt_reasoning
        )
        
        if Config.L4_PRINT:
            output = (
                "\n" + "=" * 50 + "\n" +
                "RoundRobin Bot - Chain-of-thought reasoning:\n" +
                "-" * 50 + "\n" +
                f"{self.reasoning_output}\n" +
                "=" * 50 + "\n"
            )
            print(output)


    async def _final_decision(self):
        """
        Based on the previously provided reasoning, make the final decision.
        This asynchronous LLM call asks the bot to select the superior response.
        """
        prompt = (
            "CURRENT DOCUMENT:\n\n"

            f"{self.bot1.document}\n\n"

            "SECTION INSTRUCTIONS:\n\n"

            f"{self.bot1.L2_instruction}\n\n"

            "CURRENT SECTION:\n\n"

            f"{self.bot1.section}\n\n"

            "SUBSECTION INSTRUCTIONS:\n\n"

            f"{self.bot1.L3_instruction}\n\n"

            "CURRENT SUBSECTION:\n\n"

            f"{self.bot1.subsection}\n\n"

            "SUBSECTION BLOCK 1:\n\n"

            f"{self.bot1.subsection_draft}\n\n"

            "SUBSECTION BLOCK 2:\n\n"

            f"{self.bot2.subsection_draft}\n\n"

            "REASONING:\n\n"

            f"{self.reasoning_output}\n\n"

            "You have been provided with the responses of two L3 bots above, i.e., "
            "subsection block 1 and subsection block 2. You just reasoned about the strengths and weaknesses of "
            "each in terms of the quality of the subsection and adherence to the instructions. "
            "Now, decide which response is better. Respond with '1' if the first response is better, "
            "'2' if the second response is better, or 'TIE' if both responses are equally good."
        )
        
        decision_output = await llm_call(
            prompt=prompt,
            system_prompt=self.system_prompt_decision
        )
        
        if "1" in decision_output and "2" not in decision_output:
            self.result = 1
        elif "2" in decision_output and "1" not in decision_output:
            self.result = 2
        elif "tie" in decision_output.lower():
            self.result = 0
        else:
            # In case the output is ambiguous, default to a tie.
            self.result = 0
        
        if Config.L4_PRINT:
            output = (
                "\n" + "=" * 50 + "\n" +
                "RoundRobin Bot - Final Decision:\n" +
                "-" * 50 + "\n" +
                f"{self.result}\n" +
                "=" * 50 + "\n"
            )
            print(output)
        
        self.done = True


    async def step(self):
        """
        Execute one full evaluation step of the round-robin competition.
        This method first gathers detailed reasoning from the evaluation bot and then makes a final decision.
        
        Returns:
            int: 1 if bot1 wins, 2 if bot2 wins, or 0 for a tie.
        """
        llm_call_sequence = [
            self._reason_competition,
            self._final_decision
        ]

        if self.done:
            raise RuntimeError("L3 round robin bot called after being marked done.")
        if self.current_llm_call_index >= len(llm_call_sequence):
            raise RuntimeError("L3 round robin bot invalid call sequence index.")

        await llm_call_sequence[self.current_llm_call_index]()

        self.current_llm_call_index += 1
